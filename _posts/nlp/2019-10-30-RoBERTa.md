---
layout: article_post
title:  "RoBERTa"
categories: nlp 
excerpt_separator: <!--more-->
tags: BERT RoBERTa dynamic-masking 
---

近幾年，隨著`ELMo`, `BERT`, `GPT`, `XLNet`等超大型模型在NLP任務上達到很好的成績，在許多資料集的Leaderboard上也都霸佔前幾名的位置:

- GLUE: [LeaderBoard](https://gluebenchmark.com/leaderboard/)
- RACE: [LeaderBoard](http://www.qizhexie.com/data/RACE_leaderboard.html)
- SQuAD: [LeaderBoard](https://rajpurkar.github.io/SQuAD-explorer/)

`RoBERTa`這篇論文是使用原始論文BERT的架構，但在訓練的過程做了一些改變，例如：`增大batch-size`、`動態遮罩(Dynamic Masking)`等改變，當然還有使用更大更多的訓練資料，因為作者認為原始的BERT是訓練不足的(undertrained)，所以這篇論文主要是採取各種不同的優化方法來增進BERT的效能
就如RoBERTa的名字: Robustly optimized BERT approach，經過各種優化方法後，能夠提升許多效能

<!--more-->

### 背景
---

由於訓練像ELMo, GPT, BERT, XLNet此類的超大模型，每次訓練都非常昂貴(computationally expensive)，所以也限制了我們調整參數的數量，同時也限制我們對模型的提出究竟能有多少進步的了解
於是，RoBERTa的作者群從幾個面向去改善訓練BERT的過程

1. 對於超參數所造成的影響有更嚴謹的研究
2. 提高訓練時間(Longer Training Time)
3. 增大訓練批集(Bigger Batches)
4. 使用更長的序列做訓練(Training on Longer Sequence)
5. 動態產生MLM所使用的遮罩(mask)
6. 使用更大的訓練資料(CC-NEWS)

### 貢獻
---

- 提供一系列訓練策略(training strategies)讓下游任務(downstream tasks)能有更好的表現
- 使用新的資料集(CC-NEWS)
- 研究顯示使用MLM來進行訓練是不錯的選擇(competitive choice)

#### Static vs. Dynamic Masking

比較原本`靜態遮罩(static masking)`跟改進的`動態遮罩(dynamic masking)`，Roberta改進了原本靜態遮罩，並且也使用動態遮罩的方法，並進行比較。
`靜態遮罩`：原始BERT在資料預處理(Data Preprocessing)的步驟就使用了遮罩，而RoBERTa為了避免因此每筆資料在訓練過程都使用到同樣的遮罩，讓每筆資料使用10種不同遮罩，而RoBERTa總共訓練了40epochs，所以每筆資料會有4次使用到同樣的遮罩
`動態遮罩`：動態遮罩則是在開始模型訓練之前（不是在資料預處理時），動態產生一個遮罩模式(masking pattern)，RoBERTa的實驗顯示，兩種靜態遮罩的方式效果差不多，而動態遮罩大致上比靜態遮罩稍微好一點

### Model Input Format and Next Sentence Prediction
---

對於`NSP(Next Sentence Prediction)`的功用，存在著不一致(discrepancy)的說法，Devlin et al.(2019)表示如果移掉NSP會讓模型效能變低，但也有另一派人(Lample and Conneau, Yang et al., Joshi et al., 2019)對NSP提出了質疑。RoBERTa採取了四種不同方式來測試

1. SEGMENT-PAIR + NSP: 在BERT原始論文中的設定，包含一對segments，可以有多個句子
2. SENTENCE-PAIR + NSP: 包含一對sentences，但由於長度會小於512，所以增加batch size使得接近(1)的整體大小
3. FULL-SENTENCES
4. DOC-SENTENCES

(3)會使用到跨文件的句子(並插入一個特殊的token來表示跨文件)，而(4)則是限制在同一份文件中，但在靠近文件結尾的地方，input長度可能會變得小於512，此時會提升Batch-size，但也因此使得batch-size變得不固定，在餘下的論文中使用(3)來進行實驗

### Training with large batches
---

{% include widget/excerpt.html text="We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase." %}

所以將batch-size調高是能提升效能的，BERT的原始論文中使用的batch-size是256，在這邊改成使用2K跟8K，因為batch-size調高也意味著運算的step是減少的，所以整體的計算資源是相同的，此外，更大的batch-size能更容易被平行化運算(easier to parallelize via distributed data parallel training)，經過實驗後，發現提高batch-size至8K能夠減低MLM的困惑度(Perplexity)

### Text Encoding
---

原始的論文中使用了`BPE(Byte-Pair Encoding)`來建立一個介於詞跟字之間的詞表，而在Byte-Pair中原本基本單位為unicode，而這邊使用Radford et al.(2019)所提出的方法，使用Byte當作基本單位，如此一來對於任何的input都能夠被編碼成BPE，並且學習到更大的BPE詞表，原始的BPE的詞表約為30K，新方法的詞表約為50K


### Reference
---

1. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)

