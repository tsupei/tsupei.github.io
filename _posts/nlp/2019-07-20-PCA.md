---
layout: article_post
title:  "PCA"
categories: nlp math
excerpt_separator: <!--more-->
tags: PCA eigen-vector eigen-value data-visualization
---

PCA是一個非常有名的降維方法，我認為通過降維我們可以得到很多好處

1. 資料視覺化: 在視覺化的時候通常是投影到二維的平面或三維的空間，所以需要降維投影
2. 提取特徵: 將維度高轉到維度低本身就是一個資訊壓縮的過程，所以我們可以期待有跟像CNN提取特徵的效果
3. 加快速度: 資料的維度會大大影響模型的運算速度

但是降維的過程免不了損失資訊量，PCA即是以`損失最小Variance`的想法，所以損失的全局資料量應是最少的

此外，PCA原理簡單、且計算速度很快，我們只需要求共變異數矩陣的`特徵向量(eigen vector)`及`特徵值(eigen value)`就能算出投影軸

那就讓我們來看看PCA是怎麼運作的吧!

<!--more-->

### Outline
---

1. Theory
2. Pros and Cons
3. Data Visualization
4. Conclusion
5. Reference

### Theory
---

假設我們有這些二維的資料點，如果想要降維到一維的線上會怎麼選擇呢?

可以發現:選擇A線會比選擇B線來得好，在B線上投影點都擠在一起了，而B線保留較多這些點離散的值

而`變異數(Variance)`就是拿來衡量點跟點之間離散的指標，`PCA`就是要最大化降維後資料集的變異數

假設我們現在要將$\vec{x} \; \; $投影在向量$\vec{v} \; \; $的方向，假設$\vec{v} \; \; $是單位向量，我們可以得到在$\vec{v} \; \; $方向的scalar為

$$\vec{x}\vec{v}=\left \| x \right \|\left \| v \right \|cos\theta$$


寫成矩陣表示

$$v^{T}\mathbf{x}$$

我們可以對所有的點做投影，然後求投影後所有點的變異數總和為多少

$$\sigma^{2} =\frac{1}{n}\sum (v^{T}x_{i}-\mu)^2$$

$$=\frac{1}{n}\sum (v^{T}{x}'_{i}-0)^2$$

這一步我們對所有的點扣掉平均數，而對左右平移是不影響變異數的

$$=\frac{1}{n}\sum (v^{T}{x}'_{i})^2$$

$$=\frac{1}{n}\sum (v^{T}{x}'_{i})(v^{T}{x}'_{i})^{T}$$

$$=\frac{1}{n}\sum v^{T}{x}'_{i}({x}'_{i})^{T}v$$

而我們可以發現到裡面有共變異數矩陣，於是可以寫成這個形式

$$=v^{T}Cv$$


所以，我們把變異數寫成了這個形式，但我們要求的是，變異數的最大值

$$v = \underset{v\in R^{dim}, \ \left \| v \right \|=1}{argmax}\sigma^2 =  \underset{v\in R^{dim}, \ \left \| v \right \|=1}{argmax}v^{T}Cv$$

通過[拉格朗日乘數](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)，我們可以將一個有n個變數與k個約束條件的最佳化問題轉換為一個解有n + k個變數的方程式組的解的問題

$$f(v, \lambda)=v^{T}Cv - \lambda(\left \| v \right \|-1)= v^{T}Cv - \lambda(v^{T}v-1)$$

接下來，分別對$$\lambda$$以及$$v$$做偏微分可以得到

$$\frac{\partial f(v, \lambda)}{\partial v} = 0, 2Cv-2 \lambda v=0, Cv=\lambda v$$

$$\frac{\partial f(v, \lambda)}{\partial \lambda} = 0, v^{T}v-1=0, \left \| v \right \|=1$$

看到這邊，發現我們只要求共變數矩陣$$C$$的特徵向量以及特徵值，特徵向量即為投影軸單位向量$$v$$

而我們把$$Cv=\lambda v$$再帶回到變異數的公式

$$\sigma^{2} = v^{T}Cv = v^{T} \lambda v = \lambda v^{T} v = \lambda $$

可以求出，特徵值就是我們的變異數

### Pros and Cons
---

從上面的數學推理，我們可以知道，整個PCA的過程，我們只須求共變異數矩陣$$C$$的特徵向量及特徵值

假設該資料是$$k$$維度的向量，我們可以得到$$k$$組特徵值和特徵向量

注意，因為$$C$$是`實對稱矩陣`，所以我們可以得知每個特徵向量是[彼此正交的](https://zh.wikipedia.org/wiki/%E5%B0%8D%E7%A8%B1%E7%9F%A9%E9%99%A3)

所以，我們將一組資料進行`PCA`操作後，可以依據變異數大小進行排序，並且選擇對應的投影軸進行投影(降維)

舉例來說，假如我們要把一群三維的向量，投影到二維平面進行視覺化，經過`PCA`後，我們應可以得到三組特徵向量及特徵值

然後選擇變異數最高的兩個投影軸組成的平面進行投影

`PCA`的運算速度非常快，但是在高維度資料，往往降到二維三維視覺化的時候，`變異數的比例`太低，可以看作資料量已經保留不多`離散的程度了`

所以，若是高維度的資料的話，我們可以借助`T-SNE`來呈現

### Data visualization
---

這邊我們會實際帶大家看一個例子，假設我們有一群資料，總共分成三類，我們透過了某種演算法進行了分類後，將其投影在平面上呈現

可以看出，`累積變異數比例(communative variance ratio)`從三維降到二維是幾乎無損的，可以表示二維的圖充分表現出了這些點的分布

### Conclusion
---

在這篇文章中，我們了解了最常見的降維方法`PCA`的一些應用以及簡單易懂的數學推理，更嚴謹的數學可能牽扯到更多線性代數的知識，但在[線代啟示錄](https://ccjou.wordpress.com/2013/04/15/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/)有很詳細的講解

我們也了解到`PCA`的優點在於快速、且適合低維度(五維度以下)的視覺化呈現，也可以用來配合其他方法，來增快速度，例如可以先使用`PCA`將高維度資料降到大約`50`維度左右，再使用`T-SNE`，[在這篇](https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)講解中有很好的實例可以參考

通常我們會說，能夠保留`80%`的`變異數(variance)`就算足以表示該資料的分布

### Reference
---

1. [PCA教學](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71)
2. [線代啟示錄](https://ccjou.wordpress.com/2013/04/15/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/)
3. [PCA配合TSNE資料視覺化](https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)

