---
layout: article_post
title:  "Dropout"
categories: cs nlp
tags: dropout regularization
excerpt_separator: <!--more-->
highlight: true
---

`Dropout`這個概念是在2014年被提出的，它的概念其實很簡單，在訓練的時候，隨機將一些神經元關閉，這樣避免神經元之間過度依賴(prevents units from co-adapting too much)，並在Inference時將所有神經元開啟，這樣可以輕鬆估計各個不同小的神經網路的平均值，使用`dropout`可以大幅降低overfitting的可能

<!-- more -->

### 原理

{% include widget/photo.html url="https://lh3.googleusercontent.com/mdxPl7PIc3xYSdu5dhOGthfFTzdYTwaqFbHgOtrLLzirt7d9n0xa2Xc4raHkkpSArMDWmYhhF4_r2doG_hJxMlOPqBkO5yTudYjL2lcoRVeTSnr9JRg5STZa-koovWY9keqaTcbOvKsKqXkdeePBhCgt69OPNPZGY-wUZZlqqukQzSaf5nP8UxTOM39sbc_y_rQYQoeRGq1momNbIU-dfOaREzbq-7WaRykb8DoPRJGw7SJWJKhKivNvTNwfPGhWFEtuXHUiiFVMtxXTAPRzmbQbxkCisyjip7crXMFKW4NWqVrPfmpHXBJxCV5itbtalxwdeoQprip3_fJdYb8ATE5uJuxNguR9eyKYCDTRzmk4d-CwOZsaBVVA39LkrifWeBiFjQ1QAXZtH4nYWNK8nOwNJUUmlECQA2VjTVmUJLCtYWdWH-9m9hheuG93Sh8XDQX5OktcBxuBVAP4_eLSbBkxn9LtZ5IKA0j3f1gLKRBpKS7mhrkJVj874qxUwSCrS3o8J1NkAeW-2HP1xVvUVm9pxw6JjQC3v3IOU0pRmeuYBiuAPnOcP2k4lJ8XHsWZG4CDvCNG7C9HMvcJaJZkaw2eOq_92-jV7vRFxQNXjpyOH-oYgSplnKVggogSC5wHwbZ0D4kST5MphMXSfjJXNYoP-GlajwIdW7oAiDo7yMh7lXv405cEXBw=w1324-h690-no" description="Source: ref.[2]. 左邊為一般神經網路，右邊為使用Dropout的神經網路"%}

乍聽之下很簡單的原理，用一個機率$p$來開啟神經元，但其實背後做到了`model combination`, `weight sharing`的動作。假設我們有`n`顆神經元，每顆都可能開或不開，因此我們的神經網路的可能總共為$2^{n}$種，我們稱每個小的神經網路為`thinned network`，所以在我們訓練途中，其實是在訓練$2^{n}$個 thinned network ，並且由於各個 thinned network 之間又是使用同個權重，因此可以看作權重的分享(weight sharing)

{% include widget/photo.html url="https://lh3.googleusercontent.com/Oc-wH73dro75ugfol2FqYLUeuD3J_yQx7PC8hs7LmzoD-qTA5HmiDYOCFPEIaeWE5LLJfSjMaC1GZyNaV7_LWaaFYkxp6uZ22dsunD-Ux2Ka3zliTuoWvcmHldNPDXMaSkQchpFN5vMKsKymMZQG02VAyXaLPX1f8D_f-uOPQHmZbu1Xt2dEYMkVqf3bwXdYgTN_ttAjVZau2hY92EJbmBshogujs2M_daafSffvYeLK0vXIYNCV6a_ENIX0Zd-n9xt_zg8D0BeSjVFDdV_WoBJoGBsT_OJUg2Y055v8FxrH5r6qfxbM3JSItE-xMb-wwvMTMr7DWcA37vp8Z_7hEtcTOkwHbzUil9-s8He07iJxDu7uuuLPtlQe5Lo-3KzpW837UDDJdUBNoRGVVoBsNSG9mbVvAOA7ycAJRGX0S-Bp0vD0S9J6EBRWO1GcTQgp7nMy8fDxWVp9aqwdhZpLK-KEMympGP1aRUPDCTdAn1ht9GDXOUfOpoCX0CFRn11cQUibFoMdxlqwiskGGp0ogdjYW_fgwOj9BYgmP__9ZavGB9-343uAgfgRUcXeSFqUKJi8ge8IWRAN3HNwfcaRbErVi7my4sWHFGX5XS0ba5NxcbHaQ7mL0RV-i-XrrB6UaBbX6JhphLisd3kl3wA8gwLWvnfnRq91YWR6-JCbsmbODIjG5O2cs8M=w1340-h548-no" description="Source: ref.[2]. 在使用或測試時，將權重乘上機率p，讓我們能得到單一的神經網路"%}

在`test time`的時候，理論上我們要平均$2^{n}$個 thinned network 的預測，但不可能這樣做，我們透過將神經元權重`scale-down`的動作，也就是將每個神經元的權重乘以它被開啟的機率$p$，這使得我們可以將$2^{n}$個 thinned network 合併成單一個神經網路


### 公式

這邊直接將原論文的段落引述出來，因為一些符號的意義直接看原文定義是最精準的

Consider a neural network with $L$ hidden layers. Let $$ l \in \left \{ 1, \cdots , L \right \} $$ index the hidden layers of the network. Let $z^{(l)}$ denote the vector of inputs into layer $l$, $y^{l}$ denote the vector of outputs from layer $l$ ($y^{0} = \mathbf{x} $ is the input). $W^{l}$ and $\mathbf{b^{(l)}}$ are the weights and biases at layer $l$. The feed-forward operation of a standard neural network can be described as (for $$ l \in \left \{ 1, \cdots , L \right \} $$ and any hidden unit $i$)

$$ z_{i}^{(l+1)} = w_{i}^{l+1} y^{l} + b_{i}^{(l+1)}, $$

$$ y_{i}^{(l+1)} = f(z_{i}^{(l+1)}), $$

where f is any activation function, for example, sigmoid function

With dropout, the feed-forward operation becomes

$$ r_{j}^{(l)} \sim \textit{Bernoulli} (p),$$

$$ \tilde{y}^{(l)} = r^{(l)} \ast y^{(l)},$$

$$ z_{i}^{l+1} = w_{i}^{(l+1)} \tilde{y}^{(l)} + b_{i}^{(l+1)},$$

$$ y_{i}^{(l+1)} = f(z_{i}^{(l+1)}).$$


### Reference

1. [講解非常清楚的知乎: 深度学习中Dropout原理解析](https://zhuanlan.zhihu.com/p/38200980)
2. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
