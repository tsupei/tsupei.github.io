---
layout: article_post
title:  "ALBERT"
categories: nlp 
excerpt_separator: <!--more-->
tags: BERT ALBERT 
---

今天要整理的這篇ALBERT，利用一些技術減少原先BERT中的參數，並且改進在BERT中使用的NSP，提出了`SOP Loss`有效地提升了下游任務的表現。BERT雖然在很多NLP的任務上取得成功，但是由於其巨大的模型架構，很難被應用在講求速度的實際應用上，像是聊天機器人，所以近期有許多研究都是針對怎麼讓BERT更小，怎麼讓BERT訓練更快等等。ALBERT的預訓練模型以及相關程式碼都已經開源出來了，所以讓我們好好了解一下ALBERT，並且試試看拿它來取代原先我們模型中BERT的部分

<!--more-->

### 快速總結
---
ALBERT利用了`參數共享`、`矩陣分解`等技術大大減少模型參數，利用改進的SOP Loss取代NSP Loss提升了下游任務的表現，但是BERT的層數並未減少，因此推理時間(Inference Time)還是沒有得到改進。不過，參數減少的確使得訓練變快，ALBERT可以擴展到比BERT更大的模型(ALBERT-xxlarge)，因此得到了更好的表現

### Abstract
---
1. Two parameter-reduction techniques
2. self-supervised loss that focuses on modeling inter-setence coherence
3. establishes new SOTA on the GLUE, RACE, SQuAD benchmarks

近期的大型預訓練模型帶來一系列突破性的發展，尤其是在Reading Comprehension這一塊，RACE test在2017原始論文被提出時，當時的SOTA僅有44.1%的準確度(Accuracy)，但到了現在的ALBERT已能達到89.4%，進步了高達45.3%

這些大型模型往往擁有幾百萬甚至到幾十億的參數，這使得訓練的成本非常高，目前通常透過`模型平行化(model parallelization)`及聰明的記憶體管理(clever memory management)來解決，但並沒有解決在分佈式運算(distributed learning)中溝通成本(communication overhead)的問題

#### Two Parameters Reduction Techniques

1. factorized embedding parameterization
2. cross-layer parameter sharing

ALBERT使用這兩種減少參數的技術，跟BERT-large相同的設置下，可以減少近18x的參數，訓練速度快了1.7x，同時，這樣的方法也有扮演著正規化(regularization)的角色，也有助於讓訓練更加穩定，並幫助模型更加泛化

#### Self-supervised loss for sentence-order prediction(SOP)

SOP主要是為了抓到句子間的關係(inter-sentence coherence)，用來改善原始BERT論文中的NSP loss，在BERT中，NSP loss預測下一個句子是否被置換成其他文章中的句子

### Factorized embedding parameterization
---
在BERT模型中，隱藏層長度(hidden-layer size)$H$等於字嵌入長度(Embedding size)$E$，所以當$H$越大，$E$越大，那麼嵌入層(embedding layer)的參數大小就會越大$ \left ( V \times E \right )$，但其實這些參數在訓練過程中只有少量會被更新(sparsely updated)

所以，ALBERT不直接將原本的one-hot vectors映射到hidden space size of H，而是分解成兩個矩陣，原本參數數量為$V \times H$，分解成兩步驟則減少為$V \times E + E \times H$，當$H$的數量很大的時候，這樣的作法能夠大幅降低參數數量

---
$V \times E = 30000 \times 768 = 23,040,000$

$E \left ( V + H \right ) = 256 * (30000+768) = 7,876,608$

舉個例子，當我們$V$為30000，$H$為768，$E$為256，分解成兩個矩陣後，參數從約2300萬降低到780萬左右

---

### Cross-layer parameter sharing
---

也就是直接共享某些權重，像是在每層之間共享FFN、共享Attention、共享所有參數等等，作者這邊分析了每一層input及output的`L2 distance`及`cosine similarity`，並觀察到了相比於BERT-large所呈現的曲線更加平滑，作者認為這表示`cross-layer parameter sharing`起到的穩定模型參數的效果

{% include widget/photo.html 
url="https://miro.medium.com/max/3176/1*PucmbYvzAaATQc8jqZkEtg.png"
description="test"
%}

### Inter-sentence coherence loss
---

NSP Loss是預測兩個segments是否在原始資料中為連續的兩段話，正樣本直接拿從訓練集裡面拿兩段接續的話，而負樣本則是從不同的文件中取兩段話，以相同的機率去取樣正負樣本。原本NSP是被設計來使BERT能夠加強下游任務的表現，但之後有些研究(Yang et al, 2018; Liu et al., 2019)剔除NSP後反而在一些任務上得到更好的表現

作者推測NSP的缺失應是該任務難度不夠，NSP任務去分析的話，可以說它包含兩個部分：topic prediction及coherence prediction，這邊我認為：由於正負樣本的兩個segments來自不同文件，通常代表來自不同主題，因此很大一部分透過主題預測就能解決NSP，也因此句子之間的推論關係並沒有學得很好。

作者提出SOP，不同於NSP產生負樣本的方法是取自不同文件，而是直接將正樣本的兩句話對調，這樣強迫模型專注在coherence prediction上，根據實驗顯示，NSP無法解決SOP的問題，但SOP卻能夠在一定程度上解決NSP的問題

{% include widget/photo.html 
url="https://miro.medium.com/max/3222/1*IctSWhH-jVKgYFBc-WNfMQ.png"
description="可以看出以SOP訓練後，在NSP上還是能有不錯的表現，反之則否"
%}



### Experiment
---

#### Setup and Comparision

基本上的設定都跟BERT相同，但引進了一些後來用來改進BERT的技巧，像是n-gram masking(Joshi et al, 2019)、LAMB optimizer(You et al., 2019)

### Ablation Experiment
---

很多研究的會通過Ablation(消融)來證明每個改進方法都是有用的，這邊針對factorized embedding parameterization及cross-layer parameter sharing做消融研究

- Factorized Embedding Parameterization

{% include widget/photo.html 
url="https://miro.medium.com/max/3170/1*QfJms8hiNTrY1VjcQS3Nvw.png"
description="上排是原先BERT-style架構，下排則是ALBERT-style，可以看到在下排表現最好的反而是E=128，且模型參數低於上排近8x"
%}

- Cross-layer Parameter Sharing
{% include widget/photo.html 
url="https://miro.medium.com/max/3516/1*vDdosUVwarapIUDCC9U-KQ.png"
description="我們可以發現大部分的drop都來自於shared-FFN，shared-attention在E=768只有小幅度降低(-0.7)，在E=128甚至小幅度提升(+0.1)"
%}

- Sentence Order Prediction(SOP)
 {% include widget/photo.html 
url="https://miro.medium.com/max/3222/1*IctSWhH-jVKgYFBc-WNfMQ.png"
description="可以看出SOP確實在下游任務中有提昇表現（優於NSP"
%}


### Adding Data and Remove Dropout
---

ALBERT到這邊都是使用跟BERT相同的訓練資料，但是增加訓練資料可以期待會增進模型表現，於是加上STORIES dataset後得到總和約157G的原始資料。另外，訓練到1M步的時候，發現模型還是沒對訓練集overfit，所以直接把dropout移除，結果大幅提升了在MLM上驗證集的accuracy

{% include widget/photo.html 
url="https://miro.medium.com/max/3134/1*PRcRlHmAW5pA0-naM7EyMA.png"
description="Adding data and Removing dropout increase Dev accuracy"
%}


### Discussion
--- 
1. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://openreview.net/forum?id=H1eA7AEtvS)
