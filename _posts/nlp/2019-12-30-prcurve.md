---
layout: article_post
title:  "ROC curve / PR curve"
categories: nlp
excerpt_separator: <!--more-->
tags: Evaluation machine-learning 
---

在一個二分類模型中，我們的模型通常不會直接輸出0,1直接預測出分類，而是對每個分類輸出一個機率，例如加上softmax對各分類輸出機率，這樣讓我們能夠自己設定一個門檻(threshold)來決定機率大於多少時我們判定為正樣本，反之為負樣本。而 `ROC Curve` 和 `PR Curve` 可以幫助我們分析這樣的 probablistic forcast

<!--more-->

#### Terminology
---

- `Skill` — 我們稱一個模型是 skillful，表示模型更能夠給定一個正樣本更高的機率，反之負樣本更低的機率

- `No-Skill` — 沒有 skill 的模型，在任何的門檻下，給正負樣本的機率都為0.5，我們從模型中無法獲得任何有用資訊，在 ROC 曲線中以左下到右上的對角線表示

#### Points
---

1. `ROC Curve` 適合類別平均的情況，而 `PR Curve` 適合於類別不平均的情況
2. `AUC (Area under curve)` 可以當作一個判斷整體模型能力(skill)的指標
3. 可以直接比較不同模型在不同Threshold下的Skil

### ROC Curve(Receiver Operating Characteristic Curve)
---

ROC 曲線以 FPR 為 X 軸，TPR為 Y 軸，每一個點代表設定不同的門檻值所得到的不同的 FPR 及 TPR ，最後繪製成一條曲線。建議可以參考我另一篇文章所介紹的混淆矩陣，以下會再介紹如何計算出 FPR 及 TPR

#### 偽陽性率：FPR (False Positive Rate)

FPR表示成 `1-特異度` 而`特異度(specificity)`意指正確判斷出負樣本，故特異度越高、FPR越低，模型越能夠正確判斷負樣本、表現越好

$$ \textit{Specificity} = \frac{ \textit{True Negatives} }{ ( \textit{True Negatives} + \textit{False Positives})} $$

$$ \textit{FPR} = 1 - \textit{Specificity} = \frac{\textit{False Positives}}{( \textit{False Positives} + \textit{True Negatives} )} $$

#### 真陽性率：TPR (True Positive Rate)

TPR又稱為敏感度(Sensitivity)，它也是我們熟知的召回率(Recall)，也就是正確判斷出正樣本，故TPR越高則模型越能夠正確判斷正樣本、表現越好

$$ \textit{TPR} = \textit{Sensitivity} = \frac{\textit{True Positives}}{ ( \textit{True Positives} + \textit{False Negatives} )} $$

### 曲線下面積：AUC(Area under curve)
---

{% include widget/excerpt.html text="When using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming ‘positive’ ranks higher than ‘negative’) — from wikipedia" %}

AUC(曲線下面積)，所代表的意義為隨機抽取一個正樣本，分類器會正確判斷為正樣本的機率高於判斷為負樣本的機率，所以 AUC 越高則分類器正確率會越高。而如何計算出 AUC 有幾種逼近求近似值的方法，有興趣的讀者可以參考[維基百科](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)更深入了解

#### Problem of AUC

1. AUC 常常被應用於機器學習來比較不同模型的優劣，然而其存在一些問題
估計方法所造成的問題[1](https://academic.oup.com/bioinformatics/article/26/6/822/244957)[2](https://semanticscholar.org/paper/1bab3a1af6d1fbad02385d13b64b05045110c86b)[3](https://doi.org/10.1007%2Fs10994-009-5119-5)
2. ROC AUC 將整條 ROC curve 歸納到一個單一的值上，忽略了不同系統之間的 Tradoff（不同門檻值）

#### (0, 1), (0, 0), (1, 1)

**(0,1)**這個點代表 FPR 為 0 ，TPR 為 1，代表能夠完美預測，也就是只要把門檻設定為該點的門檻，即得到一個完美的預測器

**(0,0)**這個點代表 FPR 為 0，TPR 為 0，若將門檻設定為1，那麼TP, FP都為0

**(1,1)**這個點代表 FPR 為 1，TPR 為 1，若將門檻設定為0，那麼FP, TN都為0

#### Example

```console
# -- Algorithm --
# No skill   : ROC AUC=0.500
# Algorithm 1: ROC AUC=0.843
```

{% include widget/photo.html url="https://miro.medium.com/max/2560/1*WAgW7LiQ0U8L0Jd22gpJyw.png" description="計算ROC曲線範例" %}

#### 怎麼從ROC Curve來看出哪個模型表現較好

我們已知(0,1)這個點特異度(Specificity)及敏感度(Sensitivity)為1，是完美預測的點，因此曲線越往(0,1)上凸表示該模型整體有較好的表現，如果ROC Curve A 曲線整體大於或包含 ROC Curve B，那麼A算法可以說是表現優於B算法

也可以利用AUC來概略地衡量模型整體表現，根據[這篇文章指出](https://estat.pixnet.net/blog/post/61795603-roc曲線-(receiver-operating-characteristic-curve))

1. $AUC = 0.5$ (no discrimination 無鑑別力)
2. $0.7 \leq AUC \leq 0.8$ (acceptable discrimination 可接受的鑑別力)
3. $0.8 \leq AUC \leq 0.9$ (excellent discrimination 優良的鑑別力)
4. $0.9 \leq AUC \leq 1.0$ (outstanding discrimination 極佳的鑑別力)

### PR Curve(Precision-Recall Curves)
---

PR 曲線以 Recall 為 X 軸，Precision 為 Y 軸，每一個點代表設定不同的門檻值所得到的不同的 Recall 及 Precision ，最後繪製成一條曲線。建議可以參考我另一篇文章所介紹的 [Recall 及 Precision](https://medium.com/nlp-tsupei/precision-recall-f1-score簡單介紹-f87baa82a47)

這邊特別注意的是，Recall 及 Precision 都適合用於類別不平均的資料集，因為在算式中並沒有考慮 True Negatives (TN) ，只專注於正確判斷正樣本，因此，就算資料集中負樣本的數目遠大於正樣本，Recall 及 Precision仍是有效的參考指標。反之，FPR 則會受到影響，當我們負樣本很多，模型若全部預測為負樣本，會得到 FPR = 0，但這樣的模型並非好的模型

#### AUC vs. F-measure

和 ROC AUC 一樣，曲線下面積可以代表該模型整體在各門檻下的表現，而F-measure則是針對特定某個門檻值的 Recall 及 Precision 所計算出來的調和平均數

#### (1,1)完美預測

(1,1) 代表 Precision, Recall = 1 也就是完美預測，因此我們的PR曲線越往右上角凸起則代表更好的模型表現，反之越平則代表越差

{% include widget/photo.html url="https://miro.medium.com/max/2560/1*zkBY910fW0Np3QNCTyNHqA.png" description="PR Curve Example" %}

我們將不同門檻值所算出來的值計算 F1-score 也繪製成一條線

{% include widget/photo.html url="https://miro.medium.com/max/2560/1*2y7-uswGAlAGru3Dfx4NKg.png" description="F1-scores of different thresholds" %}

### Conclussion
---

ROC 曲線同時考慮了正例以及負例，因此適用於評估分類器整體的效能，而 PR 曲線則專注於正例，若是在類別平均且正例及負例的判斷都重要的情況下，ROC 曲線是不錯的選擇

由於 ROC 曲線的 X 軸使用到了 FPR，在類別不平均的情況下(負樣本較多)，使得 FPR 的增長會被稀釋，會導致 ROC 曲線呈現出過度樂觀的結果，因此在類別不平均的情況下，PR 曲線會是較好的選擇

### Reference
---

1. [How to Use ROC Curves and Precision-Recall Curves for Classification in Python — Jason Brownlee](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)
2. [ROC曲線/維基百科](https://zh.wikipedia.org/wiki/ROC曲线)
3. [sklearn.metrices.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)
4. [sklearn.metrices.precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)
5. [机器学习之类别不平衡问题 (2) — — ROC和PR曲线 — wdmad](https://zhuanlan.zhihu.com/p/34655990)
6. [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/)
