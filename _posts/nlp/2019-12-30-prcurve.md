---
layout: article_post
title:  "深入介紹PR曲線以及ROC曲線(A deep dive into PR-curve and ROC curve)"
categories: nlp
excerpt_separator: <!--more-->
tags: evaluation machine-learning pr-curve roc-curve 
---

在一個二分類模型中，我們的模型通常不會直接輸出0,1直接預測出分類，而是對每個分類輸出一個機率，例如加上sigmoid function對各分類輸出機率，這樣讓我們能夠自己設定一個門檻(threshold)來決定機率大於多少時我們判定為正樣本，反之為負樣本。而 `ROC Curve` 和 `PR Curve` 可以幫助我們分析在設定不同的門檻值(threshold)，對於模型的表現如何，進而選擇適合的門檻值，以及分析模型的好壞。

<!--more-->

#### TL;DR;
---

1. `ROC Curve` 適合類別平均的情況，而 `PR Curve` 適合於類別不平均的情況
2. `AUC (Area under curve)` 可以當作一個判斷整體模型能力(skill)的指標
3. 可以直接比較不同模型在不同Threshold下的Skill

### ROC Curve(Receiver Operating Characteristic Curve)
---

ROC 曲線以 FPR 為 X 軸，TPR為 Y 軸，每一個點代表設定不同的門檻值所得到的不同的 FPR 及 TPR ，最後繪製成一條曲線。建議可以參考我另一篇文章所介紹的[混淆矩陣](https://tsupei.github.io/nlp/2019/10/25/cm.html)，以下會再介紹如何計算出 FPR 及 TPR

#### 偽陽性率：FPR (False Positive Rate)

FPR表示成 `1-特異度` 而`特異度(specificity)`意指正確判斷出負樣本，故特異度越高、FPR越低，模型越能夠正確判斷負樣本、表現越好

$$ \textit{Specificity} = \frac{ \textit{True Negatives} }{ ( \textit{True Negatives} + \textit{False Positives})} $$

$$ \textit{FPR} = 1 - \textit{Specificity} = \frac{\textit{False Positives}}{( \textit{False Positives} + \textit{True Negatives} )} $$

#### 真陽性率：TPR (True Positive Rate)

TPR又稱為敏感度(Sensitivity)，它也是我們熟知的召回率(Recall)，也就是正確判斷出正樣本，故TPR越高則模型越能夠正確判斷正樣本、表現越好

$$ \textit{TPR} = \textit{Sensitivity} = \frac{\textit{True Positives}}{ ( \textit{True Positives} + \textit{False Negatives} )} $$

### 曲線下面積：AUC(Area under curve)
---

{% include widget/excerpt.html text="When using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming ‘positive’ ranks higher than ‘negative’) — from wikipedia" %}

AUC(曲線下面積)，所代表的意義為隨機抽取一個正樣本，分類器會正確判斷為正樣本的機率高於誤判斷為負樣本的機率，所以 AUC 越高則分類器正確率會越高。而如何計算出 AUC 有幾種逼近求近似值的方法，有興趣的讀者可以參考[維基百科](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)更深入了解

#### Problem of AUC

1. AUC 常常被應用於機器學習來比較不同模型的優劣，然而其存在一些問題
估計方法所造成的問題[1](https://academic.oup.com/bioinformatics/article/26/6/822/244957)[2](https://semanticscholar.org/paper/1bab3a1af6d1fbad02385d13b64b05045110c86b)[3](https://doi.org/10.1007%2Fs10994-009-5119-5)
2. AUROC(Area under ROC curve) 將整條 ROC curve 歸納到一個單一的值上，忽略了不同系統之間的 Tradoff（不同門檻值）

#### Points of ROC curve

我們可以藉由觀察曲線中一些極端的點，來理解ROC曲線

`(0,1)`這個點代表 FPR 為 0 ，TPR 為 1，代表能夠完美預測，也就是只要把門檻設定為該點的門檻，即得到一個完美的預測器

`(0,0)`這個點代表 FPR 為 0，TPR 為 0，若將門檻設定為1，意思是完全沒有正樣本，那麼TP, FP都為0

`(1,1)`這個點代表 FPR 為 1，TPR 為 1，若將門檻設定為0，意思是完全沒有負樣本，那麼FP, TN都為0

#### Example

```console
# -- Algorithm --
# No skill   : AUROC=0.500
# Algorithm 1: AUROC=0.843
```

{% include widget/photo.html url="https://miro.medium.com/max/2560/1*WAgW7LiQ0U8L0Jd22gpJyw.png" description="計算ROC曲線範例" %}

#### 怎麼從ROC Curve來看出哪個模型表現較好

我們已知(0,1)這個點特異度(Specificity)及敏感度(Sensitivity)為1，是完美預測的點，因此曲線越往(0,1)上凸表示該模型整體有較好的表現，如果ROC Curve A 曲線整體大於或包含 ROC Curve B，那麼A算法可以說是表現優於B算法

也可以利用AUC來概略地衡量模型整體表現，雖沒有絕對的指標，但根據[這篇文章指出](https://estat.pixnet.net/blog/post/61795603-roc曲線-(receiver-operating-characteristic-curve))

1. $AUC = 0.5$ (no discrimination 無鑑別力)
2. $0.7 \leq AUC \leq 0.8$ (acceptable discrimination 可接受的鑑別力)
3. $0.8 \leq AUC \leq 0.9$ (excellent discrimination 優良的鑑別力)
4. $0.9 \leq AUC \leq 1.0$ (outstanding discrimination 極佳的鑑別力)

### PR Curve(Precision-Recall Curves)
---

PR 曲線以 Recall 為 X 軸，Precision 為 Y 軸，每一個點代表設定不同的門檻值所得到的不同的 Recall 及 Precision ，最後繪製成一條曲線。建議可以參考我另一篇文章所介紹的 [Recall 及 Precision](https://tsupei.github.io/nlp/2019/10/28/fscore.html)

這邊特別注意的是，Recall 及 Precision 都適合用於類別不平均的資料集，因為在算式中並沒有考慮 True Negatives (TN) ，只專注於正確判斷正樣本，因此，就算資料集中負樣本的數目遠大於正樣本，Recall 及 Precision仍是有效的參考指標。反之，FPR 則會受到影響，當我們負樣本很多，模型若全部預測為負樣本，會得到 FPR = 0，但這樣的模型並非好的模型，所以ROC curve比較容易受到不平均的樣本集(unbalanced dataset)影響。

#### AUC vs. F-measure

曲線下面積(AUC)可以代表該模型整體在各門檻下的表現，而F-measure則是針對特定某個門檻值的 Recall 及 Precision 所計算出來的調和平均數

#### (1,1)

`(1,1)` 代表 Precision, Recall = 1，這是最理想的情況，因為我們的模型不但可以每個預測的點都正確(Precision = 1)，而且也將預測出所有可能的正樣本(Recall = 1)。

我們可以從X軸來理解，X軸是從`Recall=0`開始，也意味著`threhold=1`，所以沒有樣本被預測為正樣本。

而到了最右側`Recall=1`，可能意味著`threshold=0`，所以每個樣本都猜正樣本。

因此我們的PR曲線越往右上角凸起則代表整體更好的模型表現，因為，反之越平則代表越差。

### Conclussion
---

ROC 曲線同時考慮了正例以及負例，因此適用於評估分類器整體的效能，而 PR 曲線則專注於正例，若是在類別平均且正例及負例的判斷都重要的情況下，ROC 曲線是不錯的選擇

由於 ROC 曲線的 X 軸使用到了 FPR，在類別不平均的情況下(負樣本較多)，使得 FPR 的增長會被稀釋，會導致 ROC 曲線呈現出過度樂觀的結果，因此在類別不平均的情況下，PR 曲線會是較好的選擇

### Reference
---

1. [How to Use ROC Curves and Precision-Recall Curves for Classification in Python — Jason Brownlee](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)
2. [ROC曲線/維基百科](https://zh.wikipedia.org/wiki/ROC曲线)
3. [sklearn.metrices.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)
4. [sklearn.metrices.precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)
5. [机器学习之类别不平衡问题 (2) — — ROC和PR曲线 — wdmad](https://zhuanlan.zhihu.com/p/34655990)
6. [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/)
