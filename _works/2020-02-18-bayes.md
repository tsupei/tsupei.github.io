---
layout: article_work
title:  "Naive Bayes Classifier"
categories: cs
tags: cs bayes language-model unsupervised-learning 
excerpt_separator: <!--more-->
---

常常我們會使用複雜的`神經網路`來訓練模型進行像分類的動作，不過使用`監督式(supervised)學習`的方法，我們必須有標註過後的資料才能訓練，這篇文章我們介紹，如何在不使用標註資料，而是像網路隨處可搜集到的語料，像是`PTT`來完成`主題分類`、`關鍵字預測`、`發現新詞`等功能

<!--more-->

### 介紹
---

首先讓我們回顧一下`Bayes Theorem`

$$ P(A \mid B) = \frac{P(B \mid A) P(A)}{ P(B)} $$

一篇文章該屬於哪一類，我們可以寫成

$$ P(C \mid w_{1}, w_{2}, ..., w_{n}) = \frac{P(C) P(w_{1}, w_{2}, ..., w_{n} \mid C)}{P(w_{1}, w_{2}, ..., w_{3})}$$


而`Naive Bayes Classifier(樸素貝葉斯分類器)`之所以稱為`Naive`，是因為假設各個屬性間是獨立的，在文章中則是指每個字跟別的字互相獨立

$$ P(w_{1} \mid w_{2}) = P(w_{1}) $$

假設我們有多個類的主題，而要判斷文章是屬於哪一類的主題

$$ P(C \mid w_{1}, w_{2}, ..., w_{n}) = \frac{P(C) P(w_{1}, w_{2}, ..., w_{n} | C)}{P(w_{1}, w_{2}, ..., w_{n})} $$

`因為是同一篇文章，所以分母是一樣的，於是我們得出`

$$ P(C \mid w_{1}, w_{2}, ..., w_{n}) \propto P(C) P(w_{1}, w_{2}, ..., w_{n} \mid C) $$

根據`Naive`假設各字之間獨立，可得出

$$ P(C \mid w_{1}, w_{2}, ..., w_{n}) \propto P(C) P(w_{1} \mid C) P(w_{2} \mid C) ... P(w_{n} \mid C) $$

這麼一來，我們可以得出，最基本的以`Unigram`為基礎的貝葉斯分類器公式，我們只要得出各個主題下的機率，比較誰的機率最大，即可求出最有可能的分類

$$ \underset{c_{i}}{argmax} P(C_{i}) P(w_{1} \mid C_{i}) P(w_{2} \mid C_{i}) ... P(w_{n} \mid C_{i}) $$

關於 $P(w_{n} \mid C_{i})$ 如何求得，我們使用`Unigram`來做估計，寫成以下式子

$$ P(w_{i}) = \frac{\textit{Count}(w_{i})}{ \sum_{i=1}^{n} \textit{Count}(w_{i})}$$

所以我們對每一類，都會去計算每個字$w_{i}$的對應機率$P(w_{i})$，然後可以計算出對應的`似然值(Likelihood)`

$$P(C \mid w_{1}, w_{2}, ..., w_{n}) = \prod_{i=1}^{n} P(C \mid w_{i})$$

因此，其實貝葉斯分類器在進行的就是一種`最大似然估計(MLE)`，而通常我們會轉成對數計算

$$ \sum_{i=1}^{n} log(P(C \mid w_{i})) $$

### 語言模型
---

不過，通常我們不會使用`Unigram`，而是使用`Bigram`, `Trigram`，差別就是每個字會依賴於`前k`個字

$$ \textit{Bigram} : P(w_{i} \mid w_{i-1}) $$

$$ \textit{Trigram} : P(w{i} \mid w_{i-1}, w_{i-2} ) $$

此外，我們這是從`前看到後`，我們也可以定義從`後看到前`的`k-gram`語言模型

### 實驗範例
___

我們對一個輸入句子 `電腦每種顯卡的驅動程式都不同嗎`，進行貝葉斯分類，並用`熱點圖(heatmap)`來顯示不同主題下每個 $P(C_{j} \mid w_{i})$ 的對數機率

{% include widget/photo.html url="https://i.imgur.com/osCdvSU.png" description="三種不同主題下的對數機率熱點圖" %}

可以看出，在電腦類(computer)的地方，對於像`驅動`、`程式`、`電腦`這類的關鍵字，會賦予極高的機率，相比於其他分類而言，因此我們可以得知這句話，是該分於電腦類

利用這個道理，我們也可以抓出關鍵字，像這個例子，在哪些字可以明顯看出有些關鍵字在不同類別的機率落差較大，像是`驅動`、`程式`、`電腦`這些地方，這些可以視為屬於該類別的關鍵字


